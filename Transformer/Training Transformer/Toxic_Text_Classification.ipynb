{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Text Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65xHjZlAUVO3",
        "outputId": "4dd4e52d-77f6-497a-83b2-f5daa5de1aa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.util import minibatch\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "3GAtzKoRok0P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/nishithsharma9/NLUProject.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jrdilfe_HP4",
        "outputId": "a1d6c95d-b613-4502-a4df-6a146be1dbd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLUProject'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 46 (delta 5), reused 36 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n",
            "Checking out files: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = \"./model\" #enter path where you want to save the model\n",
        "datapath = \"/content/NLUProject/Data/ToxicTextClassifierData\""
      ],
      "metadata": {
        "id": "l3I2mE2636Pt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "textcat = nlp.create_pipe(\"textcat\", config={\n",
        "                                        \"exclusive_classes\": True,\n",
        "                                        \"architecture\": \"simple_cnn\"})"
      ],
      "metadata": {
        "id": "28BZcyemrjv2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGETIVE\")\n",
        "nlp.add_pipe(textcat, last=True)"
      ],
      "metadata": {
        "id": "6NNysRIqo7ys"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(datapath + \"/train.csv\")"
      ],
      "metadata": {
        "id": "pCmFoYmbqUTO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['label'] = dataset['toxic']+dataset['severe_toxic']+dataset['obscene']+dataset['threat']+dataset['insult']+dataset['identity_hate']\n",
        "dataset=dataset.drop(['id','toxic', 'severe_toxic', 'obscene','threat','insult','identity_hate'], axis=1)\n",
        "dataset['label']=dataset['label'].astype(bool).astype(int)"
      ],
      "metadata": {
        "id": "ld335gUbq0dt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_train = dataset['comment_text'].values\n",
        "labels_train = dataset['label'].values"
      ],
      "metadata": {
        "id": "xVgOs8iUq7J0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the training data\n",
        "labels_cats = [{'cats': {\"POSITIVE\": not bool(label), \"NEGETIVE\": bool(label)}} for label in labels_train] # 0=positive,1=negetive\n",
        "data = list(zip(texts_train,labels_cats))\n",
        "\n",
        "# train the model\n",
        "optimizer = nlp.begin_training()\n",
        "losses = {}\n",
        "for epoch in range(5):\n",
        "    random.shuffle(data)\n",
        "    batches = minibatch(data,size=10)\n",
        "    for batch in batches:\n",
        "        texts,labels = zip(*batch)\n",
        "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "    print(losses)\n",
        "    \n"
      ],
      "metadata": {
        "id": "0fUnIepIpACU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bf31cb-7eb6-40c3-8555-8b17fb16e65a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'textcat': 12.810838584005651}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk(modelpath)"
      ],
      "metadata": {
        "id": "bwjeA2bBbwH_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use this saved text classifier from now\n",
        "savednlp =  spacy.load(modelpath)"
      ],
      "metadata": {
        "id": "XnH4GmOZcTgm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Biological anthropology, also known as physical anthropology, is a scientific discipline concerned with the biological and behavioral aspects of human beings, their extinct hominin ancestors, and related non-human primates, particularly from an evolutionary perspective.\"\n",
        "doc1 = savednlp(text1)\n",
        "doc1.cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbq6n8eX-lHj",
        "outputId": "d037ce66-ed29-4434-8eb4-7c1c4f2de71a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NEGETIVE': 0.027341464534401894, 'POSITIVE': 0.9726585149765015}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"I hate you as you are a fucking moron\"\n",
        "doc2 = savednlp(text2)\n",
        "doc2.cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLoiAM7bfc2c",
        "outputId": "cdf3b31c-e3b9-4d77-a01d-45d2b86bd79b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NEGETIVE': 1.0, 'POSITIVE': 4.042690071592858e-10}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tweets_test = pd.read_csv(datapath + \"/test.csv\")"
      ],
      "metadata": {
        "id": "aDZzpUHfecvP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tweets_test['comment_text'].values"
      ],
      "metadata": {
        "id": "pdarqRlHein9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # evaluate the model on test data\n",
        "# test_docs = list(nlp.pipe(tweets_test['comment_text'].values))\n",
        "# textcat = nlp.get_pipe('textcat')\n",
        "# scores, _ = textcat.predict(test_docs)\n",
        "# predicted_clases = scores.argmax(axis=1)\n",
        "# correct_predictions = predicted_clases==labels_test\n",
        "\n",
        "# print(\"F1 score=\", f1_score(predicted_clases,labels_test))\n",
        "# print(\"ROC AUC score = \", roc_auc_score(labels_test, predicted_clases))"
      ],
      "metadata": {
        "id": "l8OAvQWRrTPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}